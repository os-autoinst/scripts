#!/usr/bin/env python3

import argparse
import logging
import re
import requests
import subprocess
import sys
import json
from urllib.parse import urljoin, urlparse, urlunparse

logging.basicConfig()
log = logging.getLogger(sys.argv[0] if __name__ == '__main__' else __name__)
GOOD = '-'
BAD = '+'


class CustomFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):
    """Preserve multi-line __doc__ and provide default arguments in help strings."""

    pass


def parse_args():
    parser = argparse.ArgumentParser(description=__doc__, formatter_class=CustomFormatter)
    parser.add_argument('-v', '--verbose',
                        help='Increase verbosity level, specify multiple times to increase verbosity', action='count', default=1)
    parser.add_argument('--url', required=True,
                        help='The openQA test URL for which to trigger bisection investigation jobs')
    parser.add_argument('--dry-run', action='store_true',
                        help='Do not do any action on openQA')
    args = parser.parse_args()
    verbose_to_log = {
        0: logging.CRITICAL,
        1: logging.ERROR,
        2: logging.WARN,
        3: logging.INFO,
        4: logging.DEBUG
    }
    logging_level = logging.DEBUG if args.verbose > 4 else verbose_to_log[args.verbose]
    log.setLevel(logging_level)
    return args

client_args = ['openqa-cli', 'api', '--header', 'User-Agent: openqa-trigger-bisect-jobs (https://github.com/os-autoinst/scripts)']
def call(cmds, dry_run=False):
    log.debug('call: %s' % cmds)
    return subprocess.check_output((['echo', 'Simulating: '] if dry_run else []) + cmds).decode('utf-8')

def openqa_comment(id, host, comment, dry_run):
    args = client_args + ['--host', host, '-X', 'POST', 'jobs/' + str(id) + '/comments', 'text=' + comment]
    return call(args, dry_run)

def openqa_clone(cmds, dry_run, default_opts=['--skip-chained-deps', '--within-instance'], default_cmds=['_GROUP=0']):
    return call(['openqa-clone-job'] + default_opts + cmds + default_cmds, dry_run)

def fetch_url(url, request_type='text'):
    try:
        content = requests.get(url.geturl())
        content.raise_for_status()
    except requests.exceptions.RequestException as e:
        log.error("Error while fetching %s: %s"% (url.geturl(), str(e)))
        raise(e)
    raw = content.content
    if request_type == 'json':
        try:
            content = content.json()
        except json.decoder.JSONDecodeError as e:
            log.error("Error while decoding JSON from %s -> >>%s<<: %s" % (url.geturl(), raw, str(e)))
            raise(e)
    return content

def main(args):
    parsed_url = urlparse(args.url)
    investigation_url = urlparse(urlunparse((parsed_url.scheme, parsed_url.netloc, parsed_url.path + '/investigation_ajax', '', '', '')))
    log.debug('Retrieving investigation info from %s' % investigation_url.geturl())
    investigation = fetch_url(investigation_url, request_type='json')
    log.debug('Received investigation info: %s' % investigation)
    changes = { GOOD: set(), BAD: set() }
    if 'diff_to_last_good' not in investigation:
        return
    diff_to_last_good = investigation['diff_to_last_good'].split('\n')
    test_issues_re = re.compile('([+-]) +"(.*_TEST_ISSUES)" *: "([^"]*)",')
    for line in investigation['diff_to_last_good'].split('\n'):
        search = .search(line)
        if search:
            changes[search.group(1)] = set(search.group(2).split(','))
    if len(changes[BAD]) <= 1:
        # no value in triggering single-incident bisections
        return
    if not changes[BAD] or not changes[GOOD]:
        return
    removed, added = list(changes[GOOD] - changes[BAD]), list(changes[BAD] - changes[GOOD])
    log.debug('removed: %s, added: %s' % (removed, added))
    test_url = urlparse(urlunparse((parsed_url.scheme, parsed_url.netloc, '/api/v1/jobs/' + parsed_url.path.lstrip('/tests/'), '', '', '')))
    log.debug('Retrieving additional job data from %s' % test_url.geturl())
    test_data = fetch_url(test_url, request_type='json')

    job = test_data['job']
    children = job['children'] if 'children' in job else []
    parents = job['parents'] if 'parents' in job else []
    if ('Parallel' in children and len(children['Parallel']) \
        or 'Directly chained' in children and len(children['Directly chained']) \
        or 'Parallel' in parents and len(parents['Parallel']) \
        or 'Directly chained' in parents and len(parents['Directly chained'])):
        return

    log.debug('Received job data: %s' % test_data)
    test = test_data['job']['settings']['TEST']
    log.debug("Found test name '%s'" % test)
    created = ""
    for issue in sorted(added):
        log.info("Triggering one bisection job without issue '%s'" % issue)
        new = ','.join(sorted(list(changes[BAD] - set([issue]))))
        log.debug("New set of OS_TEST_ISSUES='%s'" % new)
        test_name = test + ':investigate:bisect_without_%s' % issue
        out = openqa_clone([args.url, 'OS_TEST_ISSUES=' + new, 'TEST=' + test_name, 'OPENQA_INVESTIGATE_ORIGIN=' + args.url], args.dry_run)
        search = re.compile('Created job .*(http\\S+/t[0-9]+)$').search(out)
        if search:
            log.info('Created ' + search.group(1))
            created += "* **%s**: %s\n" % (test_name, search.group(1))

    if len(created):
        comment = 'Automatic bisect jobs:\n\n' + created
        host = urlunparse((parsed_url.scheme, parsed_url.netloc, '', '', '', ''))
        res = openqa_comment(job['id'], host, comment, args.dry_run)
if __name__ == '__main__':
    main(parse_args())
