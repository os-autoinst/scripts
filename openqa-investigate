import requests
import json
import argparse
import os
import re
import time
import subprocess

errorlog = ""

def warn(message):
    print(message, file=sys.stderr)

def runcli(args):
    try:
        result = subprocess.run(args, capture_output=True, text=True, check=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        warn(f"Command {args} failed with error: {e.stderr}")
        return None

def runjq(input_data, jq_filter):
    try:
        result = subprocess.run(['jq', jq_filter], input=input_data, capture_output=True, text=True, check=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        warn(f"jq error: {e.stderr}")
        return None

def exp_retry(stop_exponent, exponent):
    if exponent == stop_exponent:
        warn(f"{stop_exponent} (re)tries exceeded")
        return False
    wait_sec = 2 ** exponent
    print(f"Waiting {wait_sec}s until retry #{stop_exponent}")
    time.sleep(wait_sec)
    return True

def runcurl(url, retries=0, verbose=False):
    for retry_exponent in range(retries + 1):
        if verbose:
            warn(f"[debug] curl: Fetching ({url})")
        try:
            response = requests.get(url)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            warn(f"curl error: {e}")
            if not exp_retry(retries, retry_exponent):
                break
    return None

def openqa_api_get(host_url, endpoint):
    headers = {'User-Agent': 'openqa-investigate (https://github.com/os-autoinst/scripts)'}
    response = requests.get(f"{host_url}/{endpoint}", headers=headers)
    response.raise_for_status()
    return response.json()

def comment_on_job(host_url, job_id, comment, force_result=None):
    if enable_force_result and force_result:
        comment = f"label:force_result:{force_result}:{comment}"
    response = requests.post(f"{host_url}/jobs/{job_id}/comments", data={'text': comment})
    response.raise_for_status()

def search_log(log_file, search_term, grep_timeout=5):
    try:
        result = subprocess.run(['grep', '-qPzo', search_term, log_file], timeout=grep_timeout, capture_output=True, text=True)
        return result.returncode == 0
    except subprocess.TimeoutExpired:
        warn(f"grep was killed, possibly timed out: cmd=>grep -qPzo '{search_term}' '{log_file}'")
        return False
    except subprocess.CalledProcessError as e:
        warn(f"grep failed: cmd=>grep -qPzo '{search_term}' '{log_file}' output='{e.stderr}'")
        return False

def label_on_issue(host_url, job_id, search_term, label, restart=False, force_result=None):
    log_file = get_log_file(job_id)
    if search_log(log_file, search_term):
        comment_on_job(host_url, job_id, label, force_result)
        if restart:
            response = requests.post(f"{host_url}/jobs/{job_id}/restart")
            response.raise_for_status()

def handle_unreviewed(testurl, log_file, reason, group_id, email_unreviewed, from_email, notification_address=None, job_data=None, dry_run=False):
    # This function has been simplified due to email sending being specific to a particular environment
    header = f"[{testurl}]({testurl}): Unknown test issue, to be reviewed\n-> [autoinst-log.txt]({testurl}/file/autoinst-log.txt)\n"
    excerpt = "Last lines before SUT shutdown:\n\n" + get_log_excerpt(log_file)

    if email_unreviewed and group_id != 'null':
        group_data = openqa_api_get(host_url, f"job_groups/{group_id}")
        group_description = group_data[0].get('description', '')
        group_mailto = re.search(r".*MAILTO: (\S+).*", group_description)
        group_mailto = group_mailto.group(1) if group_mailto else notification_address
        clone_id = job_data['job'].get('clone_id')

        if group_mailto and not clone_id:
            group_name = group_data[0].get('name')
            job_name = job_data['job'].get('name')
            job_result = job_data['job'].get('result')
            info = f"* Name: {job_name}\n* Result: {job_result}\n* Reason: {reason}\n\nIt might be a product bug, an outdated needle, test code needing adaptation or a test infrastructure related problem.\nAdding a [bugref](http://open.qa/docs/#_bug_references) that can be [carried over](http://open.qa/docs/#carry-over) will prevent these mails for this issue. If the carry-over is not sufficient, you may want to create a ticket with [auto-review-regex](https://github.com/os-autoinst/scripts/blob/master/README.md#auto-review---automatically-detect-known-issues-in-openqa-jobs-label-openqa-jobs-with-ticket-references-and-optionally-retrigger)."
            email = f"[{job_name}]({testurl})\n{header}\n{info}\n\n{excerpt}"
            subject = f"Unreviewed issue (Group {group_id} {group_name})"
            if not dry_run:
                send_email(group_mailto, email, from_email, subject)

def send_email(mailto, email, from_email, subject):
    # This function will just print the email to the console in this example
    print(f"To: {mailto}\nFrom: {from_email}\nSubject: {subject}\n\n{email}")

def get_log_file(job_id):
    # Placeholder for actual log file retrieval logic
    return f"/path/to/logs/{job_id}.log"

def get_log_excerpt(log_file):
    try:
        with open(log_file, 'r') as f:
            lines = f.readlines()
            excerpt = ""
            # Simulated logic for getting log excerpts
            for line in lines[-15:]:
                excerpt += line
            return excerpt
    except FileNotFoundError:
        return "(No log excerpt found)"

def query_dependency_data_or_postpone(job_id, job_data):
    response = requests.get(f"{host_url}/tests/{job_id}/dependencies_ajax")
    if response.status_code == 404:
        warn(f"Job {job_id} dependencies not found, postponing")
        return 142
    response.raise_for_status()
    return response.json()

def sync_via_investigation_comment(job_id, first_cluster_job_id):
    comments = requests.get(f"{host_url}/jobs/{job_id}/comments").json()
    comment_id = next((comment['id'] for comment in comments if comment['text'].startswith("Starting investigation for job")), None)
    first_comment_id = next((comment['id'] for comment in comments if comment['text'].startswith("Starting investigation for job")), None)
    if comment_id != first_comment_id:
        warn(f"Found existing comment id {first_comment_id}, assuming cluster jobs are already investigated and skipping")
        requests.delete(f"{host_url}/jobs/{first_cluster_job_id}/comments/{comment_id}")
        return 255
    return 0

def main():
    parser = argparse.ArgumentParser(description="Trigger investigation jobs")
    parser.add_argument("job_id", type=int, help="Job ID to investigate")
    parser.add_argument("additional_params", nargs="*", help="Additional parameters for job investigation")

    args = parser.parse_args()

    if dry_run == 1:
        job_state = runjq(client_get_job_state(args.job_id), ".job.state")
        if job_state == "done":
            warn("Job is already done, cannot investigate further")
            return 142

    job_data = client_get_job(args.job_id)
    job_group = runjq(job_data, ".job.group.name")
    if exclude_no_group and job_group == "null":
        warn(f"Skipping investigation of job {args.job_id} as it is not in any group")
        return 142

    if re.search(exclude_group_regex, job_group) or re.search(exclude_name_regex, runjq(job_data, ".job.test")):
        warn(f"Skipping investigation of job {args.job_id} as it is excluded by name or group")
        return 142

    dependency_data = query_dependency_data_or_postpone(args.job_id, job_data)
    if dependency_data == 142:
        return 142

    first_cluster_job_id = runjq(dependency_data, f'[.nodes[] | select([.id] | inside([{args.job_id}]))] | .[0].id')
    if sync_via_investigation_comment(args.job_id, first_cluster_job_id) == 255:
        return 255

    trigger_jobs(args.job_id, *args.additional_params)

if __name__ == "__main__":
    main()
