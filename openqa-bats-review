#!/usr/bin/env python3
"""
This script fetches previous cloned BATS jobs to check whether we can
tag a job as passed by doing a set intersection of all failed jobs.

Overview of BATS tests:
https://github.com/os-autoinst/os-autoinst-distri-opensuse/blob/master/tests/containers/bats
"""

import argparse
import itertools
import logging
import os
import subprocess
import sys
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
from urllib.parse import urlparse
from typing import List, Set

import requests
from requests.exceptions import RequestException


PASSED = "label:force_result:passed:" + os.path.basename(__file__)
TIMEOUT = 30
USER_AGENT = "openqa-bats-review (https://github.com/os-autoinst/os-autoinst-scripts)"

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
log = logging.getLogger(sys.argv[0] if __name__ == "__main__" else __name__)
session = requests.Session()


client_args = [
    "openqa-cli",
    "api",
    "--header",
    f"User-Agent: {USER_AGENT}",
]


def call(cmds: List[str], dry_run: bool = False) -> str:
    """
    Call openqa-cli
    """
    log.debug("call: %s", " ".join(cmds))
    res = subprocess.run(
        (["echo", "Simulating: "] if dry_run else []) + cmds,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        check=False,
    )
    if len(res.stderr):
        log.warning("call() %s stderr: %s", cmds[0], res.stderr)
    res.check_returncode()
    return res.stdout.decode("utf-8")


def openqa_comment(job: int, host: str, comment: str, dry_run: bool = False) -> str:
    """
    Comment a job
    """
    args = client_args + [
        "--host",
        host,
        "-X",
        "POST",
        "jobs/" + str(job) + "/comments",
        "text=" + comment,
    ]
    return call(args, dry_run)


def get_file(url: str) -> str:
    """
    Get a text file from URL
    """
    headers = {
        "User-Agent": USER_AGENT,
    }
    try:
        got = session.get(url, headers=headers, timeout=TIMEOUT)
        got.raise_for_status()
    except RequestException as error:
        log.error("%s: %s", url, error)
        sys.exit(1)
    return got.text


# Note: We use lru_cache instead of cache to support Python 3.6
@lru_cache(maxsize=None)
def get_job(url: str) -> dict:
    """
    Get a job from openQA
    """
    headers = {
        "User-Agent": USER_AGENT,
    }
    try:
        got = session.get(url, headers=headers, timeout=TIMEOUT)
        got.raise_for_status()
        data = got.json()
    except RequestException as error:
        log.error("%s: %s", url, error)
        sys.exit(1)
    return data["job"]


def grep_failures(url: str) -> Set[str]:
    """
    Look for failed testcases and return a set with the failing tests
    """
    failures = set()

    data = get_file(url)
    try:
        root = ET.fromstring(data)
    except ET.ParseError as e:
        log.error("Malformed JUnit XML file: %s (%s)", url, e)
        sys.exit(1)

    for testcase in root.iter("testcase"):
        if testcase.find("failure") is not None:
            name = testcase.get("name", "unknown")
            classname = testcase.get("classname", "")
            failures.add(f"{classname}:{name}")

    return failures


def process_logs(files: List[str]) -> Set[str]:
    """
    Process JUnit XML logs and return a set of failures
    """
    # The test for these packages have only one file: aardvark-dns & netavark
    if len(files) == 1:
        return grep_failures(files[0])
    # Use multithreading for tests like buildah, runc & skopeo (2 files) & podman (4 files)
    with ThreadPoolExecutor(max_workers=len(files)) as executor:
        return set(itertools.chain.from_iterable(executor.map(grep_failures, files)))


def resolve_clone_chain(openqa_host: str, job_id: int) -> List[int]:
    """
    Follow clones recursively and return the full chain:
    [job_id, origin_id, origin_id_of_origin, ...]
    """
    chain = []
    current: int | None = job_id
    while current:
        # We use "/details" because we'll need this information again and get_job() is cached
        job = get_job(f"{openqa_host}/api/v1/jobs/{current}/details")
        chain.append(current)
        current = job.get("origin_id")
    return chain


def main(url: str, dry_run: bool = False) -> None:
    """
    Main function
    """
    if not url.startswith(("http://", "https://")):
        url = f"https://{url}"
    urlx = urlparse(url)
    openqa_host = f"{urlx.scheme}://{urlx.netloc}"
    my_job_id = int(os.path.basename(urlx.path))

    chain = resolve_clone_chain(openqa_host, my_job_id)
    if len(chain) <= 1:
        log.info("No clones. Exiting")
        sys.exit(0)
    log.info("Processing clone chain: %s", " -> ".join(map(str, chain)))

    # Expected number of logs per testsuite
    expected = {
        "aardvark_testsuite": 1,
        "buildah_testsuite": 2,
        "conmon_testsuite": 2,
        "docker_testsuite": 4,
        "netavark_testsuite": 1,
        "podman_e2e": 1,
        "podman_testsuite": 4,
        "runc_testsuite": 2,
        "skopeo_testsuite": 2,
    }

    all_failures = []

    for job_id in chain:
        job = get_job(f"{openqa_host}/api/v1/jobs/{job_id}/details")
        url = f"{openqa_host}/tests/{job_id}"
        logs = [
            f"{openqa_host}/tests/{job_id}/file/{log}"
            for log in job["ulogs"]
            if log.endswith(".xml")
        ]
        if not logs:
            log.info("Job %s has no logs, skipping", job_id)
            continue

        testsuite = job["settings"]["TEST"]
        # We can't use str.removeprefix (added to Python 3.9) so we must index at the start
        if testsuite.startswith("container_host_"):
            testsuite = testsuite[len("container_host_") :]
        # We can't use str.removesuffix (added to Python 3.9) so we must index at the end
        if testsuite.endswith("_crun"):
            testsuite = testsuite[: -len("_crun")]

        # We have more tests on Tumbleweed
        if job["settings"]["DISTRI"] == "opensuse":
            # For buildah we also run conformance tests
            expected["buildah_testsuite"] += 1
            # For conmon we also test crun as root & rootless
            expected["conmon_testsuite"] += 2
            # For docker we also test ssh with python3-docker
            expected["docker_testsuite"] += 1
            # For podman we also test python3-podman & remoteintegration
            expected["podman_e2e"] += 3
            # For skopeo we also run Golang integration tests
            expected["skopeo_testsuite"] += 1

        if len(logs) != expected[testsuite]:
            log.info("Job %s has only %d logs, skipping", job_id, len(logs))
            continue

        failed = process_logs(logs)
        all_failures.append(failed)

    if len(all_failures) < 2:
        if not all_failures:
            log.info("No logs found in chain. Exiting")
        else:
            log.info("Only one job with logs in chain. Exiting")
        sys.exit(0)

    common_failures: Set[str] = set.intersection(*all_failures)

    if not common_failures:
        if not dry_run:
            log.info("No common failures across clone chain. Would tag as PASSED.")
        else:
            log.info("No common failures across clone chain. Tagging as PASSED.")
        print(openqa_comment(my_job_id, openqa_host, PASSED, dry_run))
    else:
        log.error(
            "Common failures found across clone chain: %s",
            "\n".join(sorted(list(common_failures))),
        )
        sys.exit(1)


def parse_args() -> argparse.Namespace:
    """
    Parse options & arguments
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("-n", "--dry-run", action="store_true", help="dry run")
    parser.add_argument("url", help="URL to openQA jobs")
    return parser.parse_args()


if __name__ == "__main__":
    opts = parse_args()
    main(opts.url, dry_run=opts.dry_run)
